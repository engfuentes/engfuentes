## üëã Hello, I'm Rodrigo Fuentes

A Chemical Engineer turned passionate Data Engineer, I discovered my love for data and automation while working in the Oil & Gas industry as a Field Engineer. Realizing the potential to optimize tasks through computing, I embarked on a self-learning journey in data science. From mastering Python and pandas to exploring machine learning libraries and PowerBI, I immersed myself in the world of data.

As a part of an intensive acceleration program, I collaborated with developers in a Scrum Framework environment, enhancing my skills in teamwork and agile methodologies. Recently, I delved into the realms of Apache Kafka and Apache Spark, adding powerful tools to my arsenal.

### üíº Professional Highlights:

- Chemical Engineer with hands-on experience in the Oil & Gas industry.
- Self-learned Python, pandas, machine learning, and PowerBI.
- Acceleration program participant, thriving in collaborative Scrum projects.
- Recent focus on mastering Apache Kafka and Apache Spark.

### üöÄ Some of the Projects:

- **Dash Financial Dashboard utilizing Apache Spark, Kafka and Airflow:**
Produced Reddit posts from wallstreetbets to a **Kafka** topic, processed them using **Spark**, extracting stock symbols and performing sentiment analysis with a **Hugging
Face** pipeline prior to store the results in **Cassandra**. Later retrieved symbols, downloaded stock data from Yahoo Finance, and computed diverse indicators with
**Spark**. The entire workflow was orchestrated with **Airflow**. Utilized **Docker** containerization to facilitate deployment and scalability.
- **Use NLP to analyze job descriptions in LinkedIn and apply accordingly:**
Implemented a script utilizing **Playwright** framework to obtain job descriptions for a chosen position and country. Analyzed the job descriptions with **spaCy** NPL library
to check if the user complies with the language, experience and technologies requirements. Information saved to a **PostgreSQL** database for further analysis. Linkedin EasyApply process is automated if the user fills the job requirements.
- **ETL and Data Analysis of Swedish Restaurants that sell pizza:**
Web scrapping of different JS rendered websites was done with **Scrapy** Framework and **scrapy-playwright** library. Restaurants data and financial information was
processed with a **Scrapy** pipeline and **Pandas**. **Seaborn** and **Plotly** were used to analyze which cities have the Pizza Restaurants with the best financial results.
- **ETL utilizing Airflow and Docker:**
Was part of a project done under **Scrum** Framework with a team of 8 developers. **Jira** was utilized for work management and **Git** and **Github** for version control. Automatized the tasks with **Apache Airflow** run in **Docker**. The extraction was executed from an **AWS PostgreSQL** Database. The transformation was realized with **Pandas**. The result file was uploaded to an **AWS S3** bucket.

### üåê Connect with Me:

- [Linkedin](https://www.linkedin.com/in/engfuentes/)

Feel free to explore my repositories and reach out for collaboration or discussion. Let's connect and innovate together!
